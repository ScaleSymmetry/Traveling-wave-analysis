{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import cupy as cp #uncomment this if cuda gpu available and cupy installed\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.optimize import curve_fit\n",
    "from pickle import dump,load,HIGHEST_PROTOCOL\n",
    "from os.path import exists\n",
    "import warnings\n",
    "from glob import glob\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###select one###\n",
    "\n",
    "###sEEG inter-contact distances have been computed using geodesic distances on surface models of cortex###\n",
    "measurement = 'sEEG' #example subjects\n",
    "\n",
    "###MEG inter-contact distances have been computed from a spheroid approximation of the sensory array###\n",
    "#measurement = 'MEG' #example subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data-files used in these examples can be downloaded from https://osf.io/48ztm/files/osfstorage#\n",
    "\n",
    "if measurement=='sEEG':\n",
    "    file_path = 'E:\\\\sensor_coordinates_distance_data_files\\\\' #path to where you put the input files listed below\n",
    "    output_path = 'E:\\\\output\\\\' #path where you want the output files to go (keep short, output files have long names!)\n",
    "    subjects = ['subject15','subject22'] #example subjects\n",
    "    geodesic_distance_files = ['subject15_depth_geodesic_distances','subject22_depth_geodesic_distances'] #file-name without '.pkl' affix\n",
    "    triangle_files = ['subject15_depth_equilateral_triplets','subject22_depth_equilateral_triplets'] #file-name without '.pkl' affix\n",
    "    coordinate_files = ['subject15_depth_coordinatesAP-IS-LR','subject22_depth_coordinatesAP-IS-LR'] #file-name without '.csv' affix\n",
    "    TS_files = ['subject15_depth_timeseries','subject22_depth_timeseries'] #file-name without '.npy' affix\n",
    "    tdel_values = [1.0,1.0] # one over the sampling frequency, in ms.\n",
    "    frequencies = [1.0, 1.148698354997035, 1.3195079107728942, 1.5157165665103982, 1.7411011265922482, \n",
    "                        2.0, 2.29739670999407, 2.639015821545789, 3.0314331330207964, 3.4822022531844965, \n",
    "                        4.0, 4.59479341998814, 5.278031643091579, 6.062866266041593, 6.964404506368995, \n",
    "                        8.0, 9.18958683997628, 10.556063286183157, 12.125732532083186, 13.92880901273799, \n",
    "                        16.0, 18.37917367995256, 21.112126572366314, 24.25146506416638, 27.857618025475986, \n",
    "                        32.0, 36.75834735990512, 42.22425314473263, 48.50293012833276, 55.71523605095197, \n",
    "                        64.0, 73.51669471981025, 84.44850628946526, 97.00586025666551] #temporal frequencies to analyze\n",
    "    min_of_range = 1.0/50.0 #m, for plotting\n",
    "elif measurement=='MEG':\n",
    "    file_path = 'E:\\\\sensor_coordinates_distance_data_files\\\\' #path to where you put the input files listed below\n",
    "    output_path = 'E:\\\\output\\\\' #path where you want the output files to go (keep short, output files have long names!)\n",
    "    subjects = ['subjectG0','subjectP9'] #example subjects\n",
    "    geodesic_distance_files = ['subjectG0_MEG_spheroid_distances','subjectP9_MEG_spheroid_distances'] #file-name without '.pkl' affix\n",
    "    triangle_files = ['subjectG0_MEG_equilateral_triplets','subjectP9_MEG_equilateral_triplets'] #file-name without '.pkl' affix\n",
    "    coordinate_files = ['subjectG0_MEG_coordinatesAP-IS-LR','subjectP9_MEG_coordinatesAP-IS-LR'] #file-name without '.csv' affix\n",
    "    TS_files = ['subjectG0_MEG_timeseries','subjectP9_MEG_timeseries'] #file-name without '.npy' affix\n",
    "    tdel_values = [3.2,3.2] # one over the sampling frequency, in ms.\n",
    "    frequencies = [1.0, 1.148698354997035, 1.3195079107728942, 1.5157165665103982, 1.7411011265922482, \n",
    "                        2.0, 2.29739670999407, 2.639015821545789, 3.0314331330207964, 3.4822022531844965, \n",
    "                        4.0, 4.59479341998814, 5.278031643091579, 6.062866266041593, 6.964404506368995, \n",
    "                        8.0, 9.18958683997628, 10.556063286183157, 12.125732532083186, 13.92880901273799, \n",
    "                        16.0, 18.37917367995256, 21.112126572366314, 24.25146506416638, 27.857618025475986, \n",
    "                        32.0] #temporal frequencies to analyze\n",
    "    min_of_range = 1.0/20.0 #m, for plotting\n",
    "print(subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more parameters\n",
    "aggregate_subjects = {} #for storing the spectra and other used parametres for later analysis\n",
    "verbose = True #used as a global variable, set to False once everything is working ok\n",
    "\n",
    "distance_units = 1000.0 #conversion of given sensor units of coordinates to metres; here mm->m\n",
    "minf = min(frequencies) #minimum frequency, used to window the Morlet wavelet inputs\n",
    "nFrequencies = len(frequencies) #number of frequencies\n",
    "\n",
    "include_normed_power = False #if False will compute the SF spectrum of the phase only\n",
    "forward_waves = True #separate standing wave components into pure traveling wave components\n",
    "nBases = 14 #number of bases to used in SVD\n",
    "N_cycles = 2 #number of cycles is the Morlet wavelet to estimate time-series of phase\n",
    "\n",
    "nonskinny_threshold = np.pi/4.0 #45 degrees; used for making triangular regions between contacts; should be as close as 60 degrees as possible without reducing the number of triangles too severely\n",
    "smallest_triangle_size = 0.01 #units of linear m; used for binning the triangles\n",
    "largest_triangle_size = 0.32 #units of linear m; used for binning the triangles\n",
    "\n",
    "N_histogram_bins = 32 #for plotting\n",
    "if verbose==False: warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main functions\n",
    "def complex_spatial_SVD(phi_cts,nBases=3,bases_only=False,use_bases=None,unit_phase_out=True): #singular value decomposition of the complex-valued phase data\n",
    "    print('Doing spatial SVD')\n",
    "    phi_Cs = phi_cts.reshape(-1,phi_cts.shape[-1])\n",
    "    if use_bases is None: #generate new bases from phi\n",
    "        u_sB,s_B,vh_BC= np.linalg.svd(phi_Cs.T,full_matrices=False) #b<B\n",
    "        s_b = s_B[:nBases] #diagonals of sigma\n",
    "        print('Percentage root variance',100.0*s_b/s_B.sum())\n",
    "        bases_sb = u_sB[:,:nBases] #left singular vectors\n",
    "        print('Percentage variance',100.0*(s_b)**2/((s_B)**2).sum())\n",
    "        bases_sb = u_sB[:,:nBases] #left singular vectors\n",
    "    else: bases_sb = use_bases #otherwise, use_bases is a previously generated basis set\n",
    "    if bases_only==False: #calculate fits, right singular vectors, and reduced rank model\n",
    "        if use_bases is None: betas_bC = (np.diag(s_b)@vh_BC[:nBases,:]) #weighted right singular vectors\n",
    "        else: betas_bC = (phi_Cs@bases_sb.conj()).T\n",
    "        betas_ctb = betas_bC.T.reshape(phi_cts.shape[0],-1,bases_sb.shape[1])\n",
    "        if unit_phase_out: \n",
    "            model_Cs = np.exp(1j*np.angle(bases_sb@betas_bC).T) #reduced rank model with unit length complex phases\n",
    "            model_cts = model_Cs.reshape(phi_cts.shape)\n",
    "            fit_C = (phi_Cs/model_Cs).mean(-1).real #for good fits, phi/model will point along the real axis\n",
    "            fit_ct = fit_C.reshape(phi_cts.shape[0],phi_cts.shape[1])\n",
    "            if use_bases is None: return bases_sb,s_b,fit_ct,betas_ctb,model_cts  #number of outputs change depending on input flags\n",
    "            else: return bases_sb,fit_ct,betas_ctb,model_cts #number of outputs change depending on input flags\n",
    "        else: \n",
    "            model_Cs = (bases_sb@betas_bC).T #reduced rank model\n",
    "            model_cts = model_Cs.reshape(phi_cts.shape)\n",
    "            if use_bases is None: return bases_sb,s_b,betas_ctb,model_cts #number of outputs change depending on input flags\n",
    "            else: return bases_sb,betas_ctb,model_cts #number of outputs change depending on input flags\n",
    "    else: return bases_sb #number of outputs change depending on input flags\n",
    "    \n",
    "def make_SF_power(dmetres_SS,binned_triplets_av,normalization_a,sigmas_b,Bases_sb):\n",
    "    #estimate wavelength and power for empirical data\n",
    "    bases_avb = Bases_sb[binned_triplets_av,:]\n",
    "    if verbose: print('bases_avb shape',bases_avb.shape)\n",
    "    r_avb = np.absolute(bases_avb) #magnitude of contribution by each vertex (over triangles, bases)\n",
    "    if verbose: print('r_avb min',r_avb.min(),'r_avb max',r_avb.max())\n",
    "    estpower_ba = r_avb.mean(1).T * normalization_a[np.newaxis,:] * sigmas_b[:,np.newaxis] #weight variance explained by average vertex contribution for each triangle and normalize for number of triangles at this size\n",
    "    if verbose: print('estpower_ba shape',estpower_ba.shape)\n",
    "    dphidx_ba,dphi_ba = dphi_dx_from_triangles(bases_avb.transpose(2,0,1),dmetres_SS,binned_triplets_av)\n",
    "    if verbose: print('dphidx_ba shape',dphidx_ba.shape)\n",
    "    distance_per_radian_ba = 1.0/dphidx_ba\n",
    "    metres_per_cycle_ba = 2.0*np.pi*distance_per_radian_ba #dmetres_SS already in metres\n",
    "    if verbose: print('metres_per_cycle_ba shape',metres_per_cycle_ba.shape)\n",
    "    return estpower_ba,metres_per_cycle_ba,dphidx_ba,dphi_ba\n",
    "\n",
    "def dphi_dx_from_triangles(phi_cav,dmetres_SS,triplets_av): #compute the rate of change of phase across triangles\n",
    "    a = dmetres_SS[triplets_av[:,0],triplets_av[:,1]]\n",
    "    b = dmetres_SS[triplets_av[:,1],triplets_av[:,2]]\n",
    "    c = dmetres_SS[triplets_av[:,2],triplets_av[:,0]]\n",
    "    ABC_v_xyz = coordinates_of_triangle_given_lengths(a,b,c) #nvd\n",
    "    ABC_vd = ABC_v_xyz[:,:,:2] #don't need zero Z coordinate\n",
    "    _ABC_vd = ABC_vd / np.linalg.norm(ABC_vd,axis=-2)[:,np.newaxis,:] #normalized area to one\n",
    "    relative_phase = np.angle(phi_cav[:,:,0:1]/phi_cav[:,:,0:3]) #relative to first index; units of radians\n",
    "    dphi_dx = []\n",
    "    dphi = []\n",
    "    for c in range(phi_cav.shape[0]):\n",
    "        dphi_dx_c = []\n",
    "        dphi_c = []\n",
    "        for n in range(phi_cav.shape[1]):\n",
    "            y = relative_phase[c,n,:,np.newaxis] #vo\n",
    "            X = ABC_vd[n,:,:] #vd\n",
    "            X1 = np.concatenate((X,np.ones((X.shape[0],1),float)),axis=1)\n",
    "            b = np.linalg.inv(X1.T@X1)@(X1.T@y) #dv,vd->dd; dv,vo->d0: dd,d0->d0\n",
    "            dphi_dx_c += [np.linalg.norm(b[:,0],axis=0)]\n",
    "            _X = _ABC_vd[n,:,:] #vd\n",
    "            _X1 = np.concatenate((_X,np.ones((_X.shape[0],1),float)),axis=1)\n",
    "            _b = np.linalg.inv(_X1.T@_X1)@(_X1.T@y) #dv,vd->dd; dv,vo->d0: dd,d0->d0\n",
    "            dphi_c += [np.linalg.norm(_b[:,0],axis=0)]\n",
    "        dphi_dx += [np.array(dphi_dx_c)]\n",
    "        dphi += [np.array(dphi_c)]\n",
    "    dphi_dx = np.array(dphi_dx)\n",
    "    dphi = np.array(dphi)\n",
    "    return dphi_dx,dphi\n",
    "\n",
    "def wavelet(data_cts,N_cycles,frequency,tdel,minf,use_cuda=False,PHname=None): #compute the phase and power using Morlet wavelets\n",
    "    #read in the files if they arlready exist\n",
    "    if PHname is not None: \n",
    "        LPname = PHname.replace('PH_','LP_')\n",
    "        print('Looking for',PHname)\n",
    "        print('Looking for',LPname)\n",
    "    if PHname is not None and exists(PHname+'.npy') and exists(LPname+'.npy'):\n",
    "        print('Reading in phase file')\n",
    "        phase_CTS = np.load(PHname+'.npy')\n",
    "        if verbose: print('phase_CTS min max',phase_CTS.min(),phase_CTS.max(),'phase_CTS shape',phase_CTS.shape)\n",
    "        print('Reading in power file')\n",
    "        logpower_CTS = np.load(LPname+'.npy')\n",
    "        if verbose: print('logpower_CTS min max',logpower_CTS.min(),logpower_CTS.max(),'logpower_CTS shape',logpower_CTS.shape)\n",
    "        return np.exp(1j*phase_CTS),np.power(10,logpower_CTS) #convert the phase to complex-valued unit vectors (phi), convert the logpower to power\n",
    "    else:\n",
    "        #data_cts is the time series data in shape cases (trials, epochs), times (samples), sensors (contacts, electrodes)\n",
    "        #N_cycles is the number of cycles in the Morlet wavelet; typically 2 in Alexander et al.\n",
    "        #frequency is the centre frequency, typically chosen from oversampled, logarithmically spaced frequncies\n",
    "        #tdel is the time interval between samples e.g. 1ms\n",
    "        #minf is the minimum frequency used in the analysis, need to ensure the final phase/power estimates all have the same number of samples\n",
    "        print('Estimating short time-series Fourier components for temporal frequency %.2fHz'%frequency)\n",
    "        nSamples = data_cts.shape[1]\n",
    "        nSensors = data_cts.shape[2]\n",
    "        N_cycles_of_samples = int(N_cycles*1000.0/(frequency*tdel))\n",
    "        #the extra samples of data required at both start and beginning of estimated phase/power values\n",
    "        PH_unpad_at_minf = int(N_cycles*500.0/(minf*tdel))\n",
    "        #the number of samples of phase/power that will be generated\n",
    "        PHSamples = nSamples - 2*PH_unpad_at_minf\n",
    "        #half the wavelet cycles at lowest frequency minus half the wavelet cycles at this frequency\n",
    "        PH_pad_at_f =  PH_unpad_at_minf - int(N_cycles*500.0/(frequency*tdel))\n",
    "        W = gausian_window(N_cycles_of_samples) #size of Morlet window\n",
    "        f_indexes = []\n",
    "        #how many sets of indexes to make? one for each sample in phase\n",
    "        for t in range(PHSamples):\n",
    "            #make a list of all data samples that are used in calculating each phase estimate\n",
    "            padded_range = range(t+PH_pad_at_f,t+PH_pad_at_f+N_cycles_of_samples)\n",
    "            f_indexes += padded_range\n",
    "        if use_cuda:  #works with cupy if available https://cupy.dev\n",
    "            f_indexes = cp.asarray(f_indexes)\n",
    "            power_cTs = cp.zeros((data_cts.shape[0],PHSamples,nSensors),float)\n",
    "            phi_cTs = cp.zeros((data_cts.shape[0],PHSamples,nSensors),complex)\n",
    "            W = cp.asarray(W)\n",
    "            for c,data_ts in enumerate(data_cts):\n",
    "                print('#',end='')\n",
    "                data_ts = cp.asarray(data_ts)\n",
    "                for s in range(nSensors):\n",
    "                    data_tw = data_ts[f_indexes,s].reshape(PHSamples,N_cycles_of_samples) #tw\n",
    "                    data_tw = data_tw - data_tw.mean(1)[:,cp.newaxis] #mean of time-series to zero\n",
    "                    t_f = 2.0*cp.pi*N_cycles*cp.arange(N_cycles_of_samples)/N_cycles_of_samples\n",
    "                    t_phi = cp.exp(1j*(t_f)) * W\n",
    "                    fourier_components = (data_tw * t_phi[cp.newaxis,:]).sum(1)\n",
    "                    phi_cTs[c,:,s] = cp.exp(1j*cp.angle(fourier_components)) #over w\n",
    "                    power_cTs[c,:,s] =  cp.absolute(fourier_components)\n",
    "            print()\n",
    "            return cp.asnumpy(phi_cTs),cp.asnumpy(power_cTs)\n",
    "        else:\n",
    "            f_indexes = np.asarray(f_indexes)\n",
    "            power_cTs = np.zeros((data_cts.shape[0],PHSamples,nSensors),float)\n",
    "            phi_cTs = np.zeros((data_cts.shape[0],PHSamples,nSensors),complex)\n",
    "            for c,data_ts in enumerate(data_cts):\n",
    "                print('#',end='')\n",
    "                for s in range(nSensors):\n",
    "                    data_tw = data_ts[f_indexes,s].reshape(PHSamples,N_cycles_of_samples) #tw\n",
    "                    data_tw = data_tw - data_tw.mean(1)[:,np.newaxis] #mean of time-series to zero\n",
    "                    t_f = 2.0*np.pi*N_cycles*np.arange(N_cycles_of_samples)/N_cycles_of_samples\n",
    "                    t_phi = np.exp(1j*(t_f)) * W\n",
    "                    fourier_components = (data_tw * t_phi[np.newaxis,:]).sum(1)\n",
    "                    phi_cTs[c,:,s] = np.exp(1j*np.angle(fourier_components)) #over w\n",
    "                    power_cTs[c,:,s] =  np.absolute(fourier_components)\n",
    "            print()\n",
    "        if PHname is not None: #i.e. looked for save files but didn't find them\n",
    "            print('Writing to',PHname+'.npy',end = ' ')\n",
    "            np.save(PHname+'.npy',np.angle(phi_cTs)) #saved as the real angle\n",
    "            print('Writing to',LPname+'.npy',end = ' ')\n",
    "            np.save(LPname+'.npy',np.log10(power_cTs)) #save the log power\n",
    "        return phi_cTs,power_cTs\n",
    "\n",
    "def gaus(x,a,x0,sigma): #define a Gaussian\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))\n",
    "\n",
    "def gausian_window(x): #make a Gaussian window\n",
    "    if type(x) is int:\n",
    "        n = x\n",
    "        x = np.arange(n)\n",
    "    else:\n",
    "        n = len(x)\n",
    "    cosine_window = 0.5*(1.0 - np.cos(2*np.pi*x/(n-1))) #approximate the window as a cosine first\n",
    "    mean = sum(x*cosine_window)/n\n",
    "    sigma = np.sqrt(sum(cosine_window*(x-mean)**2)/n)\n",
    "    popt,pcov = curve_fit(gaus,x,cosine_window,p0=[1,mean,sigma]) #turn it into a Gaussian window\n",
    "    G = gaus(x,*popt)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def save_pkl(output_path,obj,name): #the format used for saving dictionaries, lists\n",
    "    print('Saving',output_path+ name + '.pkl')\n",
    "    with open(output_path+ name + '.pkl', 'wb') as f:\n",
    "       dump(obj, f, HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pkl(output_path,name): #open previously saved dictionaries, lists\n",
    "    print('Loading',output_path + name + '.pkl')\n",
    "    with open(output_path + name + '.pkl', 'rb') as f:\n",
    "        return load(f)\n",
    "    \n",
    "def get_coordinates_and_distances(file_path,coordinate_file,distance_file,distance_units): # convenience function for grabbing sensor coordinates and cross-sensor distances\n",
    "    print('Opening coordinate file',coordinate_file)\n",
    "    coordinates_sd = np.loadtxt(file_path+coordinate_file+'.csv',skiprows=1,delimiter=',').T #native coordinate system units; A-P,I-S,L-R: anterior-posterior, inferior-superior, left-right\n",
    "    coordinates_metres = coordinates_sd/distance_units\n",
    "    dmetres_ss = load_pkl(file_path,distance_file)/distance_units #mm->m in this data\n",
    "    print('dmetres_ss min',dmetres_ss[dmetres_ss>0.0].min(),'dmetres_ss max',dmetres_ss[dmetres_ss<0.5].max())\n",
    "    if 'depth' in coordinate_file:\n",
    "        #initialize distances between contacts\n",
    "        dmetres_SS = np.linalg.norm(coordinates_metres[:,np.newaxis,:]-coordinates_metres[np.newaxis,:,:],axis=2)\n",
    "        if verbose: print('dmetres_SS min',dmetres_SS.min(),'dmetres_SS max',dmetres_SS.max())\n",
    "        print('dmetres_SS[dmetres_SS>0] min',dmetres_SS[dmetres_SS>0].min(),'dmetres_SS[dmetres_SS<0.5] max',dmetres_SS[dmetres_SS<0.5].max())\n",
    "        #grab geodesic distances\n",
    "        for s in range(dmetres_SS.shape[0]):\n",
    "            for ss in range(dmetres_SS.shape[1]):\n",
    "                dmetres_SS[s,ss] = max(dmetres_SS[s,ss],dmetres_ss[s,ss]) #take the largest of the two because geodesic can give zero for neighbouring contacts\n",
    "    elif 'MEG' in coordinate_file:\n",
    "        dmetres_SS = dmetres_ss\n",
    "    return coordinates_metres,dmetres_SS\n",
    "\n",
    "def angles_of_triangle_from_lengths(dmetres_SS,triplets_av): #used to construct approximate equilateral triangles from inter-contact distances\n",
    "    \"\"\"a, b and c are lengths of the sides of a triangle\"\"\" \n",
    "    a = dmetres_SS[triplets_av[:,0],triplets_av[:,1]]\n",
    "    b = dmetres_SS[triplets_av[:,1],triplets_av[:,2]]\n",
    "    c = dmetres_SS[triplets_av[:,2],triplets_av[:,0]]\n",
    "    # Square of lengths a2, b2, c2  \n",
    "    a2 = a**2\n",
    "    b2 = b**2\n",
    "    c2 = c**2\n",
    "    # From Cosine law  \n",
    "    alpha = np.arccos((b2 + c2 - a2) / (2 * b * c)) \n",
    "    beta = np.arccos((a2 + c2 - b2) / (2 * a * c))\n",
    "    gamma = np.arccos((a2 + b2 - c2) / (2 * a * b))\n",
    "    return alpha,beta,gamma\n",
    "\n",
    "\n",
    "def triangle_normal(triangles):\n",
    "    # The cross product of two sides is a normal vector\n",
    "    return np.cross(triangles[:,1] - triangles[:,0], \n",
    "                    triangles[:,2] - triangles[:,0], axis=1)\n",
    "\n",
    "def triangle_area(triangles):\n",
    "    # The norm of the cross product of two sides is twice the area\n",
    "    return np.linalg.norm(triangle_normal(triangles), axis=1) / 2\n",
    "\n",
    "def collate_nonskinny_triangles(dmetres_SS,nonskinny_threshold=np.pi/4.0): #make a list of triplets, with each triplet specifying an approximately equilateral triangle, where nonskinny_threshold is the smallest allowed angle\n",
    "    nSensors = dmetres_SS.shape[0]\n",
    "    if verbose: print('nSensors',nSensors)\n",
    "    if verbose: print('dmetres_SS shape',dmetres_SS.shape)\n",
    "    if verbose: print('dmetres_SS')\n",
    "    if verbose: print(dmetres_SS)\n",
    "    print('Making nonskinny triangles of contacts')\n",
    "    unique_triplets = []\n",
    "    for s in range(nSensors):\n",
    "        print('.',end='')\n",
    "        for ss in range(nSensors):\n",
    "            for sss in range(nSensors):\n",
    "                if s!=ss and ss!=sss and sss!=s and \\\n",
    "                    dmetres_SS[s,ss]>0.0 and dmetres_SS[ss,sss]>0.0 and dmetres_SS[sss,s]>0.0 and \\\n",
    "                    dmetres_SS[s,ss]<0.5 and dmetres_SS[ss,sss]<0.5 and dmetres_SS[sss,s]<0.5: #0.5m is the flag for cross-hemisphere distances in sEEG distance calculation:\n",
    "                    u = set(np.array([s,ss,sss]))\n",
    "                    if u not in unique_triplets: unique_triplets += [u]\n",
    "    print()\n",
    "    for u,un in enumerate(unique_triplets): unique_triplets[u] = np.sort(np.array(list(un))) #set becomes array\n",
    "    unique_triplets = np.array(unique_triplets)\n",
    "    print(unique_triplets.shape)\n",
    "    alpha_a,beta_a,gamma_a = angles_of_triangle_from_lengths(dmetres_SS,unique_triplets) #ss,av\n",
    "    angles_va = np.array([alpha_a,beta_a,gamma_a])\n",
    "    angles_min_a = angles_va.min(0)\n",
    "    nonskinnyity_met = angles_min_a>nonskinny_threshold\n",
    "    nonskinny_triplets = np.array(unique_triplets)[nonskinnyity_met]\n",
    "    if verbose: \n",
    "        print('unique_triplets shape',unique_triplets.shape)\n",
    "        print('nonskinnyity_met shape',nonskinnyity_met.shape)\n",
    "        print('nonskinny_triplets shape',nonskinny_triplets.shape)\n",
    "        print('nonskinny_triplets')\n",
    "        print(nonskinny_triplets)\n",
    "    return nonskinny_triplets #the indexes of contacts in each triangle\n",
    "\n",
    "def bin_triplets_linearSF(dmetres_SS,nonskinny_triplets,return_binwise=False,smallest_bin_size=0.01,largest_bin_size=0.32): #can return triangles in bins, or as a flat list ordered by triangle size\n",
    "    print('Binning triangles')\n",
    "    try:\n",
    "        #calculate areas of each triangle from geodesic distances\n",
    "        nominal_min_SF = 500.0 # cycles per metre\n",
    "        n_bins = int(largest_bin_size/smallest_bin_size) #this is silly way to get n=32\n",
    "        #get the triangle edges from the geodesic distances and triplets\n",
    "        ta = nonskinny_triplets\n",
    "        dm = dmetres_SS\n",
    "        dm_ta01 = dm[ta[:,0],ta[:,1]]\n",
    "        dm_ta12 = dm[ta[:,1],ta[:,2]]\n",
    "        dm_ta20 = dm[ta[:,2],ta[:,0]]\n",
    "        nominal_triangle_coordinates_te = coordinates_of_triangle_given_lengths(dm_ta01,dm_ta12,dm_ta20) #te: triangle x edge\n",
    "        triplet_areas = triangle_area(nominal_triangle_coordinates_te)\n",
    "        root_areas = np.sqrt(triplet_areas) #linear size of each triangle\n",
    "        order_root_areas = root_areas.argsort(0)\n",
    "        ordered_root_areas = root_areas[order_root_areas]\n",
    "        ordered_nonskinny_triplets = nonskinny_triplets[order_root_areas]\n",
    "        #fill the bins\n",
    "        nonskinny_triplets_linearSF_bins = [] #triangles assigned to each bin\n",
    "        nonskinny_triplets_a = [] #flat list of triangles assigned, ordered by triangle linear area\n",
    "        root_areas_linearSF_bins = [] #areas of triangles assigned to each bin\n",
    "        root_areas_a = [] #flat list of areas of triangles assigned to each bin\n",
    "        n_per_linearSF_bins = [] #number of triangles in each bin\n",
    "        if ordered_root_areas.shape[0]==0: return None\n",
    "        linearSF_bin_edges = [ordered_root_areas[0]] #first two edges, because n_bins+1 edges\n",
    "        linearSF_ranges_bins = [] #min and max of each bin\n",
    "        old_size_cutoff = 1.0/nominal_min_SF\n",
    "        for SF_cutoff in np.append(np.linspace(1.0/largest_bin_size,1.0/smallest_bin_size,n_bins),nominal_min_SF)[:-1][::-1]: #last value is taken first, but is already used in initialization\n",
    "            size_cutoff = 1.0/SF_cutoff\n",
    "            size_cutoff_boolean = ((ordered_root_areas<size_cutoff) * (ordered_root_areas>=old_size_cutoff)).astype(bool)\n",
    "            nThese = size_cutoff_boolean.sum(0)\n",
    "            these_nonskinny_triplets = ordered_nonskinny_triplets[size_cutoff_boolean,:]\n",
    "            these_root_areas = ordered_root_areas[size_cutoff_boolean]\n",
    "            if verbose:\n",
    "                print(old_size_cutoff,size_cutoff,nThese)\n",
    "                if nThese>0: print(these_root_areas.min(0),these_root_areas.max(0))\n",
    "                else: print('-','-')\n",
    "            if nThese>0:\n",
    "                nonskinny_triplets_linearSF_bins += [list(these_nonskinny_triplets)]\n",
    "                nonskinny_triplets_a += list(these_nonskinny_triplets)\n",
    "                root_areas_linearSF_bins += [list(these_root_areas)]\n",
    "                root_areas_a += list(these_root_areas)\n",
    "                linearSF_bin_edges += [size_cutoff]\n",
    "                linearSF_ranges_bins += [np.array([these_root_areas.min(0),these_root_areas.max(0)])] #zeroth is zero, 1st is smallest_bin_size, nth is largest_bin_size\n",
    "                n_per_linearSF_bins += [these_nonskinny_triplets.shape[0]]\n",
    "            else:\n",
    "                nonskinny_triplets_linearSF_bins += [[np.array([-1,-1,-1])]]\n",
    "                root_areas_linearSF_bins += [[-1]]\n",
    "                linearSF_bin_edges += [-1]\n",
    "                linearSF_ranges_bins += [[-1,-1]]\n",
    "                n_per_linearSF_bins += [0]\n",
    "            old_size_cutoff = 1*size_cutoff\n",
    "        if len(root_areas_a)==0: return None\n",
    "        root_areas_a = np.array(root_areas_a)\n",
    "        print('root_areas shape',root_areas.shape,'ordered_root_areas shape',ordered_root_areas.shape,'root_areas_a shape',root_areas_a.shape)\n",
    "        n_per_linearSF_bins = np.array(n_per_linearSF_bins)\n",
    "        #some book-keeping\n",
    "        nonzero_root_areas_linearSF_bins = []\n",
    "        nonzero_n_per_linearSF_bins = []\n",
    "        nonzero_nonskinny_triplets_linearSF_bins = []\n",
    "        nonzero_linearSF_bin_edges = []\n",
    "        nonzero_linearSF_ranges_bins = [linearSF_ranges_bins[0]]\n",
    "        for B,n_bin in enumerate(n_per_linearSF_bins):\n",
    "            if n_bin>0:\n",
    "                nonzero_root_areas_linearSF_bins += [root_areas_linearSF_bins[B]]\n",
    "                nonzero_n_per_linearSF_bins += [n_per_linearSF_bins[B]]\n",
    "                nonzero_nonskinny_triplets_linearSF_bins += [nonskinny_triplets_linearSF_bins[B]]\n",
    "                if len(nonzero_linearSF_bin_edges)==0: nonzero_linearSF_bin_edges += [linearSF_bin_edges[B]]\n",
    "                nonzero_linearSF_bin_edges += [linearSF_bin_edges[B+1]] #n_bins + 1 values\n",
    "                nonzero_linearSF_ranges_bins += [linearSF_ranges_bins[B]]\n",
    "        nonzero_linearSF_bin_edges[-1] = ordered_root_areas[-1] #correct the last edges\n",
    "        nonzero_linearSF_ranges_bins[-1][1] = ordered_root_areas[-1] #correct the second term of the last nonzero bin range\n",
    "        nonzero_linearSF_bin_edges = np.array(nonzero_linearSF_bin_edges)\n",
    "        nonzero_linearSF_ranges_bins = np.array(nonzero_linearSF_ranges_bins)\n",
    "        nonzero_n_per_linearSF_bins = np.array(nonzero_n_per_linearSF_bins)\n",
    "        range_a = [ordered_root_areas[0],ordered_root_areas[-1]]\n",
    "        print('nonzero_linearSF_ranges_bins')\n",
    "        print(nonzero_linearSF_ranges_bins)\n",
    "        max_of_linearSF_bins = max(nonzero_n_per_linearSF_bins)\n",
    "        normalization_linearSF_bins = -np.ones(n_per_linearSF_bins.shape,float)\n",
    "        normalization_linearSF_a = [] #'linearSF' because its based on assumption of linearly increasing SF\n",
    "        print('n_per_linearSF_bins sum',n_per_linearSF_bins.sum(),'nonzero_n_per_linearSF_bins sum',nonzero_n_per_linearSF_bins.sum())\n",
    "        for B,bin_n in enumerate(n_per_linearSF_bins): \n",
    "            if bin_n>0:\n",
    "                normalization_linearSF_bins[B] = max_of_linearSF_bins/bin_n #will eventually multiply by normalization weight\n",
    "                normalization_linearSF_a += bin_n*[normalization_linearSF_bins[B]] #grab copies equal to the number of triangles in bin\n",
    "        normalization_linearSF_a = np.array(normalization_linearSF_a)\n",
    "        print('normalization_linearSF_a shape',normalization_linearSF_a.shape)\n",
    "        if verbose: \n",
    "            print('linearSF_bin_edges')\n",
    "            print(linearSF_bin_edges)\n",
    "            print('linearSF_ranges_bins')\n",
    "            print(linearSF_ranges_bins)\n",
    "            print('n_per_linearSF_bins')\n",
    "            print(n_per_linearSF_bins)\n",
    "            print('normalization_linearSF_a')\n",
    "            print(normalization_linearSF_a)\n",
    "            print('nonzero_linearSF_bin_edges')\n",
    "            print(nonzero_linearSF_bin_edges)\n",
    "            print('nonzero_n_per_linearSF_bins')\n",
    "            print(nonzero_n_per_linearSF_bins)\n",
    "            plot_bin_counts_and_normalization(nonzero_linearSF_bin_edges,nonzero_n_per_linearSF_bins,root_areas_a,normalization_linearSF_a,SFscale=True,cut_final_bin=True)\n",
    "        if return_binwise: return n_per_linearSF_bins,nonskinny_triplets_linearSF_bins,normalization_linearSF_bins,\\\n",
    "            linearSF_ranges_bins,root_areas_linearSF_bins,linearSF_bin_edges #first array is used to avoid empty bins\n",
    "        else: return n_per_linearSF_bins,np.array(nonskinny_triplets_a),normalization_linearSF_a,range_a,root_areas_a,linearSF_bin_edges\n",
    "    except: \n",
    "        print('No binned triplets returned')\n",
    "        return None   \n",
    "\n",
    "def coordinates_of_triangle_given_lengths(a,b,c): #abstract triangle coordinates from geodesic distances\n",
    "    \"\"\"a, b and c are lengths of the sides of a triangle\"\"\"\n",
    "    A = np.array([[0]*len(a),[0]*len(a),[0]*len(a)]) # coordinates of vertex A\n",
    "    B = np.array([c,[0]*len(a),[0]*len(a)]) # coordinates of vertex B\n",
    "    #if verbose: print(A.shape,B.shape)\n",
    "    C_x = b * (b**2 + c**2 - a**2) / (2 * b * c)\n",
    "    C_y =  np.sqrt(b**2 - C_x**2) # square root\n",
    "    C = np.array([C_x,C_y,[0]*len(a)]) # coordinates of vertex C\n",
    "    return np.concatenate([[A],[B],[C]],axis=0).transpose(2,0,1) #tvd: triangle, vertex, cartesian dimension\n",
    "\n",
    "def decompose_rphi_into_forward_waves(rphi_cts,tdel,frequency): #removes the negative going component of the wave and estimates normalized velocity\n",
    "    #tdel is 1/sampling_frequency in ms\n",
    "    #frequency in Hertz\n",
    "    #rphi_cts is complex valued phase, possibly with magnitude weighting\n",
    "    print('Decompose spatial vectors of phase into pure traveling wave components')\n",
    "    one_cycle_of_samples = int(1000.0/(tdel*frequency))\n",
    "    rphi_Cs = rphi_cts.reshape(-1,rphi_cts.shape[2])\n",
    "    vel_C = [] #normalized velocity, 0 is pure SW, 1 is pure TW\n",
    "    TWf_C = [] #forward spatial component\n",
    "    for C in range(rphi_Cs.shape[0]):\n",
    "        rphi_Ts = np.exp(1j*np.linspace(-np.pi,np.pi,one_cycle_of_samples,endpoint=False))[:,np.newaxis] * rphi_Cs[C,:][np.newaxis,:]\n",
    "        u_r,s_r,vt_r = np.linalg.svd(rphi_Ts.real)\n",
    "        TWf_s = vt_r[0,:] + 1j*vt_r[1,:] #throw away the time dimension we added, only need the spatial dimension\n",
    "        TWf_C += [TWf_s[np.newaxis]]\n",
    "        vel_C += [np.array([s_r[1]/s_r[0]])[np.newaxis]]\n",
    "    vel_ct = np.concatenate(vel_C,axis=0).reshape(rphi_cts.shape[0],rphi_cts.shape[1]) \n",
    "    TWf_cts = np.concatenate(TWf_C,axis=0).reshape(rphi_cts.shape[0],rphi_cts.shape[1],rphi_cts.shape[2])\n",
    "    return vel_ct,TWf_cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting functions\n",
    "def sf_histogram(imaging_path,plot_name,output_file,list_of_estpower,list_of_wavelength,list_of_max_distances, #plot the histogram of SF\n",
    "                 max_of_range,min_of_range,N_histogram_bins,\n",
    "                 stacked=False,colourmap='RdYlBu_r',linewidth=1,ticklabelsize=10.0,axislabelsize=12.0,listwise_weights=None):\n",
    "    print('Plotting',plot_name)\n",
    "    if verbose: print('max_of_range',max_of_range,'min_of_range',min_of_range)\n",
    "    if verbose: print('1.0/max_of_range',1.0/max_of_range,'1.0/min_of_range',1.0/min_of_range)\n",
    "    cmap = cm.get_cmap(colourmap)\n",
    "    colours = []\n",
    "    for l in range(len(list_of_estpower)):\n",
    "        colours += [cmap(l/len(list_of_estpower))] #avoid division by zero\n",
    "    #colours = ['red','green','blue','magenta','cyan']\n",
    "    fig1 = figure(figsize=(5,5),dpi=90)\n",
    "    axs1 = []\n",
    "    axs1 += [fig1.add_subplot(1,1,1)]\n",
    "    axs1[-1].set_title(plot_name.replace('_',' '))\n",
    "    axs1[-1].tick_params(axis='x',which = 'both',labelsize=ticklabelsize)\n",
    "    axs1[-1].tick_params(axis='y',labelsize=ticklabelsize)\n",
    "    axs1[-1].set_xscale('log')\n",
    "    if 'sEEG' in plot_name: \n",
    "        locs = np.append(np.arange(1.0,10.0,1.0),np.arange(10.0,100.0,10.0))\n",
    "        locs = np.delete(locs,np.where(locs==9.0))\n",
    "        locs = np.delete(locs,np.where(locs==40.0))\n",
    "        locs = np.delete(locs,np.where(locs==60.0))\n",
    "        locs = np.delete(locs,np.where(locs==70.0))\n",
    "        locs = np.delete(locs,np.where(locs==80.0))\n",
    "        locs = np.delete(locs,np.where(locs==90.0))\n",
    "    elif 'MEG' in plot_name: \n",
    "        locs =  np.append(np.arange(1.0,10.0,1.0),np.arange(10.0,50.0,10.0))\n",
    "        locs = np.delete(locs,np.where(locs==9.0))\n",
    "        locs = np.delete(locs,np.where(locs==40.0))\n",
    "    axs1[-1].xaxis.set_minor_locator(ticker.FixedLocator(locs))\n",
    "    axs1[-1].xaxis.set_major_locator(ticker.NullLocator())\n",
    "    axs1[-1].xaxis.set_minor_formatter(ticker.ScalarFormatter())\n",
    "    axs1[-1].set_xlabel('Spatial frequency (cycles/metre)',fontsize=axislabelsize)\n",
    "    axs1[-1].set_ylabel('Estimated power (A.U.)',fontsize=axislabelsize)\n",
    "    if listwise_weights is None: listwise_weights = np.ones((len(list_of_estpower)),float)\n",
    "    logscale_bins = np.power(10,np.linspace(np.log10(1.0/max_of_range),np.log10(1.0/min_of_range),N_histogram_bins))\n",
    "    list_of_hist_power = []\n",
    "    list_of_hist_sf = []\n",
    "    if stacked:\n",
    "        cumulative_hist = np.zeros((N_histogram_bins-1),float)\n",
    "        for l,estpower,wavelength,case_weight in zip(range(len(list_of_estpower)),list_of_estpower,list_of_wavelength,listwise_weights):\n",
    "            wavelength = np.array(wavelength)\n",
    "            estpower = np.array(estpower)\n",
    "            sf = 1.0/wavelength\n",
    "            power_hist = np.histogram(sf,bins=logscale_bins,weights=estpower)\n",
    "            w_power_hist = case_weight * power_hist[0] #case_weight is a post-hoc way to add weighting to each curve e.g. to mimic sigma\n",
    "            cumulative_hist += w_power_hist\n",
    "            axs1[-1].plot((power_hist[1][:-1]+power_hist[1][1:])/2.0,cumulative_hist,c=colours[l],linewidth=linewidth)\n",
    "            for md in list_of_max_distances:\n",
    "                axs1[-1].plot(1.0/md,0.0,'.',color='k')        \n",
    "            list_of_hist_power += [w_power_hist]  \n",
    "            list_of_hist_sf += [(power_hist[1][:-1]+power_hist[1][1:])/2.0]     \n",
    "    else:\n",
    "        for l,estpower,wavelength,case_weight in zip(range(len(list_of_estpower)),list_of_estpower,list_of_wavelength,listwise_weights):\n",
    "            wavelength = np.array(wavelength)\n",
    "            estpower = np.array(estpower)\n",
    "            sf = 1.0/wavelength\n",
    "            power_hist = np.histogram(sf,bins=logscale_bins,weights=estpower)\n",
    "            w_power_hist = case_weight * power_hist[0]\n",
    "            axs1[-1].plot((power_hist[1][:-1]+power_hist[1][1:])/2.0,w_power_hist,c=colours[l],linewidth=linewidth)\n",
    "            for md in list_of_max_distances:\n",
    "                axs1[-1].plot(1.0/md,0.0,'.',color='k')        \n",
    "            list_of_hist_power += [w_power_hist]  \n",
    "            list_of_hist_sf += [(power_hist[1][:-1]+power_hist[1][1:])/2.0]     \n",
    "    display(fig1) \n",
    "    fig1.savefig((imaging_path+plot_name+'_'+output_file).replace(' ','_').replace('c/m','cperm')+'.jpg',facecolor='white',transparent=False,dpi=600)\n",
    "    fig1.savefig((imaging_path+plot_name+'_'+output_file).replace(' ','_').replace('c/m','cperm')+'.svg',facecolor='white',transparent=False,dpi=600)\n",
    "    fig1.clf()\n",
    "    close(fig1)\n",
    "    del fig1\n",
    "    return list_of_hist_power,list_of_hist_sf\n",
    "\n",
    "def plot_bin_counts_and_normalization(bin_edges,bin_counts,used_root_areas,normalization_a,SFscale=True,cut_final_bin=False): #for checking the triangle construction, binning, normalization\n",
    "    print('Plotting bin counts')\n",
    "    fig = figure(figsize=(5,2.5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    if SFscale:\n",
    "        ax.plot(1.0/bin_edges[1:],bin_counts,'4',color='g')\n",
    "        ax.plot(1.0/bin_edges[:-1],bin_counts,'3',color='r')\n",
    "        ax.set_xlabel('Bin edges (1/m)')\n",
    "        ax.set_ylabel('Triangle count')\n",
    "    else:\n",
    "        ax.plot(bin_edges[1:],bin_counts,'3',color='g')\n",
    "        ax.plot(bin_edges[:-1],bin_counts,'4',color='r')\n",
    "        ax.set_xlabel('Bin edges (m)')\n",
    "        ax.set_ylabel('Triangle count')\n",
    "    xlims = ax.get_xlim()\n",
    "    ax.set_xlim(xlims[0],1.2/bin_edges[1])\n",
    "    display(fig)\n",
    "    fig.clf()\n",
    "    close(fig)\n",
    "    del fig\n",
    "    print('Plotting binwise normalization')\n",
    "    fig = figure(figsize=(5,2.5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    if SFscale:\n",
    "        ax.plot(1.0/used_root_areas,1.0/normalization_a,'.',color='k',markersize=0.2) #normalization_a is divisor in SF power calculation\n",
    "        ax.set_xlabel('Triangle SF')\n",
    "        ax.set_ylabel('1.0 / normalization factor')\n",
    "    else:\n",
    "        ax.plot(used_root_areas,1.0/normalization_a,'.',color='k',markersize=0.2) #normalization_a is divisor in SF power calculation\n",
    "        ax.set_xlabel('Triangle size')\n",
    "        ax.set_ylabel('1.0 / normalization factor')\n",
    "    xlims = ax.get_xlim()\n",
    "    ax.set_xlim(xlims[0],1.2/bin_edges[1])\n",
    "    display(fig)\n",
    "    fig.clf()\n",
    "    close(fig)\n",
    "    del fig\n",
    "    \n",
    "def phase_plot(bases_sb,frequency,contact_xyz,vector_name='wave map',mag=False,figout=None): #assumes complex valued bases\n",
    "    print('Plotting phases')\n",
    "    nBases = bases_sb.shape[1]\n",
    "    cols = 3\n",
    "    rows = (nBases//3)+1\n",
    "    fig = figure(figsize=(cols*8,rows*8),dpi=200)\n",
    "    axs = []\n",
    "    if mag: alpha = np.absolute(bases_sb)/np.absolute(bases_sb).max()\n",
    "    else: alpha = np.ones(bases_sb.shape,float)\n",
    "    for b in range(nBases):\n",
    "        axs += [fig.add_subplot(rows,cols,b+1,projection='3d')]\n",
    "        axs[b].view_init(elev=90,azim=-90)\n",
    "        axs[b].set_title('Frequency %.1fHz %s %d'%(frequency,vector_name,b+1))\n",
    "        for s,xyz in enumerate(contact_xyz):\n",
    "            color = cm.hsv(np.angle(bases_sb[s,b]) / (2.0*np.pi) + 0.5) #put angles on range 0 to 1\n",
    "            axs[b].plot(xyz[0],xyz[1],xyz[2],'.',markersize=20.0,markerfacecolor=color,markeredgecolor=None,alpha=alpha[s,b])\n",
    "            axs[b].plot(xyz[0],xyz[1],xyz[2],'.',markersize=20.0,markeredgecolor=color,fillstyle='none',alpha=1.0)\n",
    "            axs[b].set_aspect('equal')\n",
    "        axs[b].set_xlabel('Anterior-Posterior')\n",
    "        axs[b].set_ylabel('Inferior-Superior')\n",
    "    fig.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "    if figout is not None: \n",
    "        fig.savefig(figout+'.png',facecolor='white',transparent=False)\n",
    "        fig.savefig(figout+'.svg',facecolor='white',transparent=False)\n",
    "    else:\n",
    "        rect = fig.patch\n",
    "        rect.set_facecolor('white')\n",
    "    display(fig)\n",
    "    fig.clf()\n",
    "    close(fig)\n",
    "    del fig\n",
    "\n",
    "def draw_triangles(contact_xyz,binned_triplets,bin_counts,file_path,title,root_areas): #for visualization of the equilateral triangles, draws in simple cartesian coordinates\n",
    "    print('Plotting triangles over size bins')\n",
    "    nBinPlots = bin_counts.shape[0]\n",
    "    cols = 3\n",
    "    rows = (nBinPlots//3)+1\n",
    "    fig = figure(figsize=(cols*10,rows*10),dpi=150)\n",
    "    axs = []\n",
    "    cumulative_bin_count = 0\n",
    "    for br in range(nBinPlots):\n",
    "        axs += [fig.add_subplot(rows,cols,br+1,projection='3d')]\n",
    "        axs[br].view_init(elev=90,azim=-90)\n",
    "        axs[br].plot(contact_xyz[:,0],contact_xyz[:,1],contact_xyz[:,2],'.',color='k')\n",
    "        if bin_counts[br]>0:\n",
    "            triplets = binned_triplets[cumulative_bin_count:cumulative_bin_count+bin_counts[br]]\n",
    "            mean_root_area = root_areas[cumulative_bin_count:cumulative_bin_count+bin_counts[br]].mean(0)\n",
    "            axs[br].set_title('Size bin %d, mean size %.3f'%(br+1,mean_root_area))\n",
    "            for t,trip in enumerate(triplets):\n",
    "                if triplets.shape[0]>1: colour = cm.cool(t/(triplets.shape[0])) #0 to 1\n",
    "                else: colour = cm.cool(0.0)\n",
    "                T = np.concatenate((trip,trip[0:1]),axis=0) #close the triangle\n",
    "                axs[br].plot(contact_xyz[T,0],contact_xyz[T,1],contact_xyz[T,2],color=colour,alpha=1.0/np.sqrt(bin_counts[br])) #density of colour on plot increases with line length\n",
    "                if t==0: axs[br].plot(contact_xyz[T,0],contact_xyz[T,1],contact_xyz[T,2],color=cm.cool(0.0),alpha=1.0)\n",
    "                elif t==triplets.shape[0]-1: axs[br].plot(contact_xyz[T,0],contact_xyz[T,1],contact_xyz[T,2],color=cm.cool(1.0),alpha=1.0)\n",
    "        axs[br].set_aspect('equal')\n",
    "        xticks = axs[br].get_xticks()\n",
    "        axs[br].set_xticks(xticks[::2])\n",
    "        zticks = axs[br].get_zticks()\n",
    "        axs[br].set_zticks(zticks[::4])\n",
    "        cumulative_bin_count += bin_counts[br]\n",
    "        set_axes_equal(axs[br])  # aspect ratio is 1:1:1 in data space\n",
    "    tight_layout()\n",
    "    fig.savefig((file_path+title).replace('c/m','cperm')+'.png')\n",
    "    fig.savefig((file_path+title).replace('c/m','cperm')+'.svg')\n",
    "    display(fig)\n",
    "    fig.clf()\n",
    "    close(fig)\n",
    "    del fig\n",
    "\n",
    "def set_axes_equal(ax): #to draw veridical relative distances in plots\n",
    "    \"\"\"\n",
    "    Make axes of 3D plot have equal scale so that spheres appear as spheres,\n",
    "    cubes as cubes, etc.\n",
    "\n",
    "    Input\n",
    "      ax: a matplotlib axis, e.g., as output from plt.gca().\n",
    "    \"\"\"\n",
    "    x_limits = ax.get_xlim3d()\n",
    "    y_limits = ax.get_ylim3d()\n",
    "    z_limits = ax.get_zlim3d()\n",
    "    x_range = abs(x_limits[1] - x_limits[0])\n",
    "    x_middle = np.mean(x_limits)\n",
    "    y_range = abs(y_limits[1] - y_limits[0])\n",
    "    y_middle = np.mean(y_limits)\n",
    "    z_range = abs(z_limits[1] - z_limits[0])\n",
    "    z_middle = np.mean(z_limits)\n",
    "    # The plot bounding box is a sphere in the sense of the infinity\n",
    "    # norm, hence I call half the max range the plot radius.\n",
    "    plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main script\n",
    "binning_success = 0 #keep track of whether triangles were successfuly constructed and binned\n",
    "for j,subject in enumerate(subjects): #loop through the participant's files\n",
    "    tdel = tdel_values[j]\n",
    "    print('Doing subject',subject)\n",
    "    #MEG, sEEG files; made using other resources not included in this code base\n",
    "    coordinate_file = coordinate_files[j]\n",
    "    distance_file = geodesic_distance_files[j]\n",
    "    timeseries_file = TS_files[j]\n",
    "    triangle_file = triangle_files[j]\n",
    "    print(triangle_file)\n",
    "    metres_sd,dmetres_SS = get_coordinates_and_distances(file_path,coordinate_file,distance_file,distance_units)\n",
    "    #save relevant paramenters with subject for later analysis\n",
    "    aggregate_subjects[subject] = {}\n",
    "    aggregate_subjects[subject]['tdel'] = tdel\n",
    "    aggregate_subjects[subject]['metres_sd'] = metres_sd\n",
    "    aggregate_subjects[subject]['dmetres_SS'] = dmetres_SS\n",
    "    nSensors = metres_sd.shape[0]\n",
    "    #look for triplet files, make them if they don't exist (slow, taking many hours for large number of contacts, so we save them after making)\n",
    "    if exists(file_path+triangle_file+'_nonskinny_threshold_%.4f.pkl'%nonskinny_threshold): nonskinny_triplets = load_pkl(file_path,triangle_file+'_nonskinny_threshold_%.4f'%nonskinny_threshold)\n",
    "    else: \n",
    "        nonskinny_triplets = collate_nonskinny_triangles(dmetres_SS,nonskinny_threshold=nonskinny_threshold)\n",
    "        save_pkl(file_path,nonskinny_triplets,triangle_file+'_nonskinny_threshold_%.4f'%nonskinny_threshold)\n",
    "    if verbose: print('nonskinny_triplets shape',nonskinny_triplets.shape)\n",
    "    #put the triangles into bins of the approximately the same linear sizes\n",
    "    binning_out = bin_triplets_linearSF(dmetres_SS,nonskinny_triplets,return_binwise=False,smallest_bin_size=smallest_triangle_size,largest_bin_size=largest_triangle_size)\n",
    "    if binning_out is not None: #if None, failed to bin the triplets with current values for nonskinny_threshold, smallest_bin_proportion, or because small number of distances\n",
    "        #housekeeping from the binning\n",
    "        binning_success += 1\n",
    "        analysis_info_string = 'nB_%d_nC_%d_nonskinny_th_%.4f_t_range_%.3f_to_%.3f'%(nBases,N_cycles,nonskinny_threshold,smallest_triangle_size,largest_triangle_size)\n",
    "        print('analysis_info_string',analysis_info_string)\n",
    "        n_per_linearSF_bins,binned_triplets_av,normalization_a,range_a,root_areas_a,linearSF_bin_edges = binning_out\n",
    "        aggregate_subjects[subject]['binned_triplets_av'] = binned_triplets_av\n",
    "        aggregate_subjects[subject]['normalization_linearSF_a'] = normalization_a #for naming consistency\n",
    "        aggregate_subjects[subject]['range_a'] = range_a\n",
    "        aggregate_subjects[subject]['root_areas_a'] = root_areas_a\n",
    "        aggregate_subjects[subject]['linearSF_bin_edges'] = linearSF_bin_edges\n",
    "        bin_counts = n_per_linearSF_bins[n_per_linearSF_bins>0]\n",
    "        do_bins_string = ''\n",
    "        min_triangle = range_a[0]\n",
    "        max_triangle = range_a[-1]\n",
    "        print('n_per_linearSF_bins',n_per_linearSF_bins)\n",
    "        print(j,binning_success,subject)\n",
    "        if verbose:\n",
    "            plot_title = triangle_file+'_nonskinny_threshold_%.4f_smallest_triangle_size_%.3f_largest_triangle_size_%.3f%s'%(nonskinny_threshold,smallest_triangle_size,largest_triangle_size,do_bins_string)\n",
    "            draw_triangles(metres_sd,binned_triplets_av,bin_counts,output_path,plot_title,root_areas_a)\n",
    "    else: continue #try the next subject\n",
    "    max_distance = dmetres_SS[dmetres_SS<0.5].max() #0.5 (metres) is used as the mask term since it is larger than the largest human head\n",
    "    max_of_range = 2*max_triangle #used for plotting\n",
    "    aggregate_subjects[subject]['max_distance'] = max_distance\n",
    "    aggregate_subjects[subject]['max_of_range'] = max_of_range\n",
    "    aggregate_subjects[subject]['min_of_range'] = min_of_range\n",
    "    timeseries_cts = np.load(file_path+timeseries_file+'.npy')\n",
    "    if verbose: print('timeseries_cts shape',timeseries_cts.shape)\n",
    "    for frequency in frequencies:\n",
    "        aggregate_subjects[subject][frequency] = {}\n",
    "        list_of_estpower = []\n",
    "        list_of_wavelength = []\n",
    "        #estimate phase\n",
    "        data_label = 'empirical'\n",
    "        print('Empirical bases from SVD')\n",
    "        PHname = \"%sPH_%.4fHz_%s_%s_%.4fminf\" %(file_path,frequency,subject,measurement,minf)\n",
    "        phi_cts,pow_cts = wavelet(timeseries_cts,N_cycles,frequency,tdel,minf,PHname=PHname)\n",
    "        if include_normed_power: \n",
    "            data_label = 'npower'\n",
    "            pow___s = pow_cts.reshape(-1,pow_cts.shape[2]).mean(0) #normalize power per each grey-matter contact\n",
    "            pow_cts = pow_cts / pow___s[np.newaxis,np.newaxis,:]\n",
    "            phi_cts = pow_cts * phi_cts #now weighted by normalized power\n",
    "        else: data_label = 'phase'\n",
    "        del pow_cts\n",
    "        # and make left singular vectors\n",
    "        bases_sb,sigmas_b,betas_ctb,reduced_cts = complex_spatial_SVD(phi_cts,nBases=nBases,bases_only=False,use_bases=None,unit_phase_out=False)\n",
    "        if verbose:\n",
    "            print('bases_sb shape',bases_sb.shape)\n",
    "            print('absolute(bases_sb) min',np.absolute(bases_sb).min(),'absolute(bases_sb) max',np.absolute(bases_sb).max())\n",
    "            phase_plot(bases_sb,frequency,metres_sd,mag=True)\n",
    "        # remove the negative wave components\n",
    "        if forward_waves: \n",
    "            vel_ct,TWf_cts = decompose_rphi_into_forward_waves(bases_sb.T[:,np.newaxis,:],tdel,frequency)\n",
    "            if verbose: print('TWf_cts shape',TWf_cts.shape)\n",
    "            print('Normalized velocity of spatial vector of phase (SW=0,TW=1)')\n",
    "            print(vel_ct.T)\n",
    "            Bases_sb = TWf_cts[:,0,:].T\n",
    "            if verbose: print('absolute(Bases_sb) min',np.absolute(Bases_sb).min(),'absolute(Bases_sb) max',np.absolute(Bases_sb).max())\n",
    "            print('Empirical traveling wave components')\n",
    "            if verbose: phase_plot(Bases_sb,frequency,metres_sd,mag=True)\n",
    "        else: Bases_sb = bases_sb\n",
    "        aggregate_subjects[subject][frequency]['Bases_sb'] = Bases_sb\n",
    "        #estimate the SF\n",
    "        estpower_ba,metres_per_cycle_ba,dphidx_ba,dphi_ba = make_SF_power(dmetres_SS,binned_triplets_av,normalization_a,sigmas_b,Bases_sb)\n",
    "        list_of_estpower += [estpower_ba.reshape(-1)]\n",
    "        list_of_wavelength += [metres_per_cycle_ba.reshape(-1)] \n",
    "        plot_name = '%s %s SF %s TF %.2fHz'%(subject,measurement,data_label,frequency)\n",
    "        aggregate_subjects[subject][frequency]['list_of_estpower'] = list_of_estpower\n",
    "        aggregate_subjects[subject][frequency]['list_of_wavelength'] = list_of_wavelength  \n",
    "        save_pkl(output_path,aggregate_subjects[subject],(plot_name+'_'+analysis_info_string).replace(' ','_'))\n",
    "        sf_histogram(output_path,plot_name,analysis_info_string,list_of_estpower,list_of_wavelength,\\\n",
    "            [max_triangle],max_of_range,min_of_range,N_histogram_bins,colourmap='cool')\n",
    "        del aggregate_subjects[subject][frequency]\n",
    "    clear_output(wait=True) #.pynb file can get very large, especially with verbose==True\n",
    "print('binning success',binning_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
